{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Case Study Tracing Evolutionary Changes in APIs - Preparation Phase",
   "id": "39e18150cfd283ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following case study aims at performing an analysis on 2 Java APIs. The first API is *JUnit4*, the dataset for which is already constructed from another study. The second API is AppCompat for which we need to manually construct the dataset and analyse the evolution.\n",
    "\n",
    "## Goal\n",
    "Our goal is to investigate the feasibility of the machine learning approach. This will be achieved via the following steps:\n",
    "- Read the Excel sheets and analyze the data\n",
    "- Extract important features such as the \"Changes\" column and then perform Natural Language Processing Techniques on them, such as tokenization\n",
    "- Train the algorithm to perform classification\n",
    "- Check the accuracy of the algorithm\n",
    "- In the final stage, be able to classify solely based on the \"Changes\" column\n",
    "First, we start with importing the necessary libraries and defining the file paths"
   ],
   "id": "c2ef68a00c9599dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Some implementation ideas/goals\n",
    " - Analyse for each category what are the most popular words, basically what infers that there is a Bug change, etc.\n",
    " - Try to predict the category of the change based on the trained data\n",
    " - At later stage, try the following: \"A really interesting question will be which changes impacting the architecture of the system are represented in the release log. This way you essentially combine the two RQs and the make even more sense.\" For example, are breaking changes represented in the release log, etc."
   ],
   "id": "9674425ccc84e094"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:58:42.487082Z",
     "start_time": "2024-12-12T13:58:42.483517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n"
   ],
   "id": "7a858da6431fc83",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:58:42.505495Z",
     "start_time": "2024-12-12T13:58:42.492284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"../resources/JUnit/JUnit - Training.xlsx\"\n",
    "sheet_name = \"JUnit\""
   ],
   "id": "e704111592465856",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After defining our path, the first thing we do is to display part of our data, in order to check if everything is working correctly.",
   "id": "3b93b6a313812eff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:58:42.602495Z",
     "start_time": "2024-12-12T13:58:42.516636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "print(data.head())"
   ],
   "id": "2c3cc6db5c1921fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Year1  Year       Date        Version RELEASE  \\\n",
      "0  +15 years  2021 2021-02-13  4.13.  4.13.2   PATCH   \n",
      "1  +15 years  2021 2021-02-13  4.13.  4.13.2   PATCH   \n",
      "2  +15 years  2021 2021-02-13  4.13.  4.13.2   PATCH   \n",
      "3  +15 years  2021 2021-02-13  4.13.  4.13.2   PATCH   \n",
      "4  +14 years  2020 2020-10-11  4.13.  4.13.1   PATCH   \n",
      "\n",
      "                                             Changes  By          .1 Type  \\\n",
      "0                                                NaN NaN         NaN  NaN   \n",
      "1  Pull request #1687: Mark ThreadGroups created ... NaN       Rules  NaN   \n",
      "2  Pull request $1691: Only create ThreadGroups i... NaN       Rules  NaN   \n",
      "3  Pull request #1654: Fix for issue #1192: NotSe... NaN  Exceptions  NaN   \n",
      "4                                                NaN NaN         NaN  NaN   \n",
      "\n",
      "             General General Category  \n",
      "0                NaN              NaN  \n",
      "1     Fix regression          Bug fix  \n",
      "2     Fix regression          Bug fix  \n",
      "3  Fix serialization          Bug fix  \n",
      "4                NaN              NaN  \n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next parts, we will focus on building the classifier.\n",
    "### 1. Tokenize the \"Changes\" column\n",
    "First we perform the tokenization of the \"Changes\" column and then we remove any stopwords, so that the output is cleaner and easier to analyse in later steps"
   ],
   "id": "3289874075af974b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:58:42.635809Z",
     "start_time": "2024-12-12T13:58:42.611612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import tokenizer model\n",
    "# Make sure to download punkt\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "data[\"Changes\"] = data[\"Changes\"].fillna(\"\")\n",
    "data[\"Tokens\"] = data[\"Changes\"].apply(word_tokenize)\n",
    "\n",
    "print(data[[\"Tokens\", \"Changes\"]].head())\n",
    "\n",
    "# Define the stopwords in a set.\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "data[\"Tokens\"].apply(lambda tokens: [word for word in tokens if word.casefold() not in stop_words])\n",
    "\n",
    "print(\"Data without stop words: \")\n",
    "print(data[\"Tokens\"].head())\n"
   ],
   "id": "6183a2610fcaca26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Tokens  \\\n",
      "0                                                 []   \n",
      "1  [Pull, request, #, 1687, :, Mark, ThreadGroups...   \n",
      "2  [Pull, request, $, 1691, :, Only, create, Thre...   \n",
      "3  [Pull, request, #, 1654, :, Fix, for, issue, #...   \n",
      "4                                                 []   \n",
      "\n",
      "                                             Changes  \n",
      "0                                                     \n",
      "1  Pull request #1687: Mark ThreadGroups created ...  \n",
      "2  Pull request $1691: Only create ThreadGroups i...  \n",
      "3  Pull request #1654: Fix for issue #1192: NotSe...  \n",
      "4                                                     \n",
      "Data without stop words: \n",
      "0                                                   []\n",
      "1    [Pull, request, #, 1687, :, Mark, ThreadGroups...\n",
      "2    [Pull, request, $, 1691, :, Only, create, Thre...\n",
      "3    [Pull, request, #, 1654, :, Fix, for, issue, #...\n",
      "4                                                   []\n",
      "Name: Tokens, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Krisi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next parts, we will focus on building the classifier.\n",
    "### 2. Stem the output\n",
    "The next step is to stem the output, the reason for doing this is to focus on the basic meaning of the word."
   ],
   "id": "fbdb6f062e586764"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T13:58:47.304025Z",
     "start_time": "2024-12-12T13:58:47.273916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Apply the stemmer\n",
    "data[\"Stemmed_Tokens\"] = data[\"Tokens\"].apply(\n",
    "    lambda tokens: [stemmer.stem(word) for word in tokens if isinstance(word, str)]\n",
    ")\n",
    "\n",
    "print(\"Stemmed tokens: \")\n",
    "print(data[[\"Tokens\", \"Stemmed_Tokens\"]].head())"
   ],
   "id": "a19fb723a52f8c74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed tokens: \n",
      "                                              Tokens  \\\n",
      "0                                                 []   \n",
      "1  [Pull, request, #, 1687, :, Mark, ThreadGroups...   \n",
      "2  [Pull, request, $, 1691, :, Only, create, Thre...   \n",
      "3  [Pull, request, #, 1654, :, Fix, for, issue, #...   \n",
      "4                                                 []   \n",
      "\n",
      "                                      Stemmed_Tokens  \n",
      "0                                                 []  \n",
      "1  [pull, request, #, 1687, :, mark, threadgroup,...  \n",
      "2  [pull, request, $, 1691, :, onli, creat, threa...  \n",
      "3  [pull, request, #, 1654, :, fix, for, issu, #,...  \n",
      "4                                                 []  \n"
     ]
    }
   ],
   "execution_count": 55
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
