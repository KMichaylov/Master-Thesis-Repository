{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Experimental Study: Tracing Evolutionary Changes in APIs\n",
    "\n",
    "## author: Kristiyan Michaylov\n",
    "## supervisor: Jacob KrÃ¼ger\n",
    "---"
   ],
   "id": "39e18150cfd283ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This case study aims to conduct an experimental analysis on Java-based API datasets. The primary examined datasets are **JUnit**, **Log4J**, **Apache Commons IO**, and **Project Lombok**, and they form the basis of the results presented in our research paper. Additionally, a combined dataset incorporating all the aforementioned APIs is constructed and evaluated to assess aggregated performance and insights.\n",
    "## Research Question (RQ)\n",
    "This case study attempts to answer the following research question:\n",
    "> To what extent can an automated machine learning technique analyze and categorise the causes of changes in Java APIs?\n",
    "## Process\n",
    "Given the RQ, the goal is to investigate the feasibility of machine learning approaches to categorise the causes of changes. We achieve this via the following overview steps:\n",
    "- Read the Excel sheets and analyze the data via visualisation techniques\n",
    "- Extract prepare and tokenise input from the *Changes* column and then perform Natural Language Processing Techniques on them, such as tokenization\n",
    "- Train the machine learning (ML) models with the aid of hyper-parameters to perform classification\n",
    "- Check the performance of the models with metrics such as accuracy, recall, precision and F1-score"
   ],
   "id": "c2ef68a00c9599dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, recall_score, precision_score, f1_score, \\\n",
    "    confusion_matrix\n"
   ],
   "id": "7a858da6431fc83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, we set the `dataset_name`, which is used as the name for diagram outputs. Then in the `file_path` the corresponding path to the dataset is provided, from where the data is read.",
   "id": "30b59fd82c716c3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# file_path = \"../resources/excel-sheets/JUnit.xlsx\"\n",
    "# sheet_name = \"JUnit\"\n",
    "dataset_name = 'JUnit'\n",
    "file_path = \"../resources/datasets/JUnit.csv\""
   ],
   "id": "e704111592465856",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After defining our path, the first thing we do is to create a Pandas dataframe which contains the data. Then to validate that the data is assigned, the first 5 rows are printed.",
   "id": "3b93b6a313812eff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    data = pd.read_csv(file_path, sep=',')\n",
    "except pd.errors.ParserError:\n",
    "    data = pd.read_csv(file_path, sep=';')\n",
    "print(data.head())"
   ],
   "id": "2c3cc6db5c1921fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Exploration and Analysis\n",
    "First, we explore and visualizing parts of the dataset. Particularly, we check which words are associated to certain classes, what is their frequency and to see how data is distributed, some popular words, noises, etc. This information would later aid in deciding which parts of the data are important and which not."
   ],
   "id": "723177d42557e041"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Wordcloud\n",
    "First we create the wordcloud to see the most common words. We only don't consider stopwords and punctuation symbols"
   ],
   "id": "33f75ba470c53dbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [str(item) for item in data['Changes']]\n",
    "# print(words)\n",
    "joined_sentences = \" \".join(words)\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=STOPWORDS,\n",
    "    max_font_size=25,\n",
    "    scale=3\n",
    ")\n",
    "\n",
    "wordcloud = wordcloud.generate(joined_sentences)\n",
    "\n",
    "fig = plt.figure(1, figsize=(15, 15))\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()\n"
   ],
   "id": "95058704dede295c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Ngrams\n",
    "We simply show Ngrams which are contiguous sequences of n words. This gives information on how the data is dispersed."
   ],
   "id": "ce6f75f17bf0947"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_n_gram(dt, n=None, stopwords='english'):\n",
    "    dt = dt.dropna().astype(str)\n",
    "    vec = CountVectorizer(ngram_range=(n, n), stop_words=stopwords).fit(dt)\n",
    "    # We get a sparse matrix corresponding to the occurrence of words per column\n",
    "    sparse_matrix = vec.transform(dt)\n",
    "    sum_words = sparse_matrix.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]\n",
    "\n",
    "\n",
    "bi_grams = get_n_gram(data['Changes'], 2)\n",
    "\n",
    "x, y = map(list, zip(*bi_grams))\n",
    "sns.barplot(x=y, y=x)"
   ],
   "id": "b3ddf2b020203e4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Text exploration\n",
    "We check the text properties such as length, most common words and distribution overall and per category. This includes stopwords since we are interested in the length of each release note."
   ],
   "id": "2ea246c2f7bd912d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_clean = data['Changes'].dropna().astype(str)\n",
    "word_count = data_clean.str.split().apply(len)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=word_count.value_counts().index, y=word_count.value_counts().values, palette='viridis',\n",
    "            hue=word_count.value_counts().index)\n",
    "plt.title('Word Count Frequency Distribution per Release Log Change')\n",
    "plt.xlabel('Number of Words in \"Changes\" Column')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(word_count, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Word Counts in \"Changes\" Column')\n",
    "plt.xlabel('Number of Words in \"Changes\" column')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "d43db028fa4cf808",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, the most common words per category are presented.\n",
   "id": "fd51658cea02a502"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def separate_into_words(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    if isinstance(text, str):\n",
    "        words_in_change = text.split()\n",
    "        return [word.lower() for word in words_in_change if re.match(r'^[a-zA-Z]+$',\n",
    "                                                                     word) and word.casefold() not in stop_words]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "category_words = data.groupby('General Category')['Changes'].apply(\n",
    "    lambda texts: [word for text in texts for word in separate_into_words(text)]\n",
    ")\n",
    "\n",
    "category_word_freq = category_words.apply(Counter)\n",
    "\n",
    "for category, freq in category_word_freq.items():\n",
    "    most_common = dict(freq.most_common(10))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(most_common.keys()), y=list(most_common.values()))\n",
    "\n",
    "    file_name = category.replace(\" \", \"_\")\n",
    "\n",
    "    plt.title(f\"Top Words in Category: {category}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # plt.savefig(f\"../resources/diagrams/top_words_{file_name}.png\", format='png')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ],
   "id": "cd4721584a371c50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next the overall most popular words are showcased.\n",
   "id": "394db93739c2f021"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_words = [word for text in data['Changes'] for word in separate_into_words(text)]\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "most_common = dict(word_freq.most_common(10))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(most_common.keys()), y=list(most_common.values()))\n",
    "\n",
    "plt.title(\"Top 10 Most Popular Words\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"../resources/diagrams/top_words_all.png\", format='png')\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "id": "9f0f2e75b2daf61c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Category distribution\n",
    "We check how are the different categories are distributed over the release log changes."
   ],
   "id": "bdc0d14c00c2d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "categories = data[\"General Category\"].dropna().astype(str)\n",
    "category_frequency = Counter(categories)\n",
    "category_names = list(category_frequency.keys())\n",
    "category_frequencies = list(category_frequency.values())\n",
    "\n",
    "output_dir = \"../resources/data-exploration\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "colors = plt.get_cmap(\"tab20\", len(category_names))\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "pie_chart = ax.pie(category_frequencies, labels=category_names, colors=colors(np.arange(len(category_names))),\n",
    "                   autopct='%1.1f%%')\n",
    "\n",
    "# ax.set_title(\"Pie Chart Distribution of Categories\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "file_path = os.path.join(output_dir, f\"pie_chart_{dataset_name}.png\")\n",
    "\n",
    "# plt.savefig(file_path, format='png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.close()\n"
   ],
   "id": "5b244d4805f17cd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categories = data[\"General Category\"].dropna().astype(str)\n",
    "category_frequency = Counter(categories)\n",
    "category_names = list(category_frequency.keys())\n",
    "category_frequencies = list(category_frequency.values())\n",
    "\n",
    "output_dir = \"../resources/data-exploration\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "colors = plt.get_cmap(\"tab20\", len(category_names))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(x=category_names, height=category_frequencies, color=colors(np.arange(len(category_names))))\n",
    "\n",
    "ax.set_title(\"Distribution of Categories\")\n",
    "ax.set_xlabel(\"Category\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "file_path = os.path.join(output_dir, f\"bar_plot_{dataset_name}.png\")\n",
    "\n",
    "plt.savefig(file_path, format='png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.close()"
   ],
   "id": "d2dc05e2f2ca3c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1 Clean the Dataset and Tokenize\n",
    "First we clean the data from non-alphabetic characters and then perform the tokenization of the \"Changes\" column. After that we remove any stopwords, so that the output is cleaner and easier to analyse in later steps"
   ],
   "id": "5f1eaed0885efff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make sure to download the following\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "data.dropna(inplace=True, subset=\"Changes\")\n",
    "print(data[\"Changes\"].head())\n",
    "\n",
    "\n",
    "def find_most_common_words(count):\n",
    "    cnt = Counter()\n",
    "    for text in data[\"Changes\"].values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "    return cnt.most_common(count)\n",
    "\n",
    "\n",
    "def remove_frequent_words(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in most_frequent_words])\n",
    "\n",
    "\n",
    "most_frequent_words = set([w for (w, wc) in find_most_common_words(10)])\n",
    "\n",
    "data[\"Changes\"] = data[\"Changes\"].apply(lambda text: remove_frequent_words(text))\n",
    "\n",
    "data[\"Tokens\"] = data[\"Changes\"].apply(word_tokenize)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "data[\"Tokens\"] = data[\"Tokens\"].apply(lambda tokens: [word.lower() for word in tokens\n",
    "                                                      if re.match(r'^[a-zA-Z]+$', word)\n",
    "                                                      and word.casefold() not in stop_words])\n",
    "\n",
    "data = data[data[\"Tokens\"].apply(lambda tokens: len(tokens) > 0)]\n",
    "\n",
    "print(data[\"Tokens\"].head())\n",
    "print(\"Data without stop words and solely alphabetical tokens: \")\n",
    "print(data[\"Tokens\"].head())\n",
    "print(data[[\"Tokens\", \"General Category\"]].to_string())\n"
   ],
   "id": "d0023c92796ea8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next parts, we will focus on preparing the data for the ML classifiers.\n",
    "### 2. Prepare the output\n",
    "The next step is to stem or lemmatize the output. The stem reduces the word to its stem, while the lemmatizer alters the word in a way which that still preserves the meaning of the word. We explore both options in our experiment."
   ],
   "id": "fbdb6f062e586764"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def stematize_or_lemmatizer_input(is_stem=True):\n",
    "    if is_stem:\n",
    "        data[\"Adapted_Changes\"] = data[\"Tokens\"].apply(\n",
    "            lambda tokens: [stemmer.stem(word) for word in tokens if isinstance(word, str)]\n",
    "        )\n",
    "    else:\n",
    "        nltk.download('wordnet')\n",
    "        data[\"Adapted_Changes\"] = data[\"Tokens\"].apply(\n",
    "            lambda tokens: [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        )\n",
    "\n",
    "\n",
    "# Here we remove all categories which have only one instance in the dataset, since this cannot work during the test split.\n",
    "# category_counts = data['General Category'].value_counts()\n",
    "#\n",
    "# categories_with_multiple_occurrences = category_counts[category_counts > 1].index\n",
    "#\n",
    "# data = data[data['General Category'].isin(categories_with_multiple_occurrences)]\n",
    "\n",
    "stematize_or_lemmatizer_input()\n",
    "print(\"Adapted tokens: \")\n",
    "print(data[[\"Tokens\", \"Adapted_Changes\"]].head())"
   ],
   "id": "a19fb723a52f8c74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Input processing\n",
    "After clearing the input, the next step is to extract features which will be suitable for the machine learning model. For this, we use TF-IDF (Term Frequency-Inverse Document Frequency).\n"
   ],
   "id": "ea09bf9e14d1b680"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def return_vectorizer(name_of_vectorizer=\"tf_idf\"):\n",
    "    if name_of_vectorizer == \"tf_idf\":\n",
    "        return TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 3))\n",
    "    elif name_of_vectorizer == \"bow\":\n",
    "        return CountVectorizer()\n",
    "\n",
    "vectorizer = return_vectorizer()\n",
    "\n",
    "print(data[\"Adapted_Changes\"].head())\n",
    "\n",
    "documents = data[\"Adapted_Changes\"].apply(lambda tokens: ' '.join(tokens) if isinstance(tokens, list) else '')\n",
    "\n",
    "# print(documents.head())\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(X.shape)\n",
    "print(X)"
   ],
   "id": "644ce44abc52786d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Labels preparation\n",
    "Once the data is prepared, the next step is to define the lables which will be used for categorization."
   ],
   "id": "e591acc2c986dbf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_encoder = LabelEncoder()\n",
    "print(data[\"General Category\"].head())\n",
    "\n",
    "y = label_encoder.fit_transform(data[\"General Category\"])\n",
    "print(y)\n",
    "# print(label_encoder.classes_)\n",
    "original_categories = label_encoder.inverse_transform(y)\n",
    "# print(original_categories)\n",
    "original_class_names = label_encoder.classes_\n",
    "category_to_label = zip(original_categories, y)\n",
    "category_to_label = [unique_tpl for unique_tpl in (set(tuple(pair) for pair in category_to_label))]\n",
    "category_to_label.sort(key=lambda x: x[1])\n",
    "print(category_to_label)\n",
    "\n",
    "class_support = np.bincount(y)\n",
    "\n",
    "classes_to_include = [cls for cls, support in enumerate(class_support) if support >= 0]\n",
    "print(classes_to_include)"
   ],
   "id": "31beeba77b5911b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Split training and testing data\n",
    "The next step involves preparing the training and testing datasets for the machine learning model. An 80/20 split is applied, allocating 80% of the data for training and 20% for testing. To ensure reproducibility of the results, a random seed of 42 is used. This guarantees that the data is split consistently across runs when using the same parameters and model configuration. Additionally, due to the imbalances in all examined datasets, a `RandomOverSampler` strategy is utilized to mitigate the problem\n",
    "\n"
   ],
   "id": "a078e98172ea5daa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(pd.Series(y_train_res).value_counts())\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(pd.Series(y_test).value_counts())\n",
    "\n",
    "print(\"Training data shape: \", X_train.shape)\n",
    "print(\"Test data: \", X_test.shape)\n"
   ],
   "id": "355bfb912371cd14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parameter Tuning\n",
    "To improve the general performance of the models, now we experiment with tuning the parameters. For this, a combination of parameters is passed and the best ones are returned."
   ],
   "id": "bec372988a1822ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "print(data['General Category'].value_counts())\n",
    "cvs = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "def return_best_params(hyper_parameter_type, model, params):\n",
    "    if hyper_parameter_type == \"Grid\":\n",
    "        search_type = GridSearchCV(estimator=model, param_grid=params, cv=cvs, n_jobs=-1, verbose=2, scoring='f1_macro')\n",
    "    elif hyper_parameter_type == \"Random\":\n",
    "        search_type = RandomizedSearchCV(estimator=model, param_distributions=params, cv=cvs, n_iter=500,\n",
    "                                         scoring='f1_macro',\n",
    "                                         n_jobs=-1, random_state=42)\n",
    "\n",
    "    search_type.fit(X_train_res, y_train_res)\n",
    "    print(search_type.best_params_)\n",
    "\n",
    "    return search_type.best_estimator_\n"
   ],
   "id": "ae4dbe3c013af0a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Train model\n",
    "After the data is split and prepared, we then train the classifier. For this we use 3 different ML models for which we check the accuracy, precision, recall and F1 score of the model. We check the following in the next code blocks."
   ],
   "id": "a8fabebed7dce91d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 6.1. RandomForrestClassifier\n",
    "\n"
   ],
   "id": "74d275f57adb5d03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "# Desired Thresholds for metrics:\n",
    "# Accuracy -> above 70%\n",
    "# Precision -> above 70%\n",
    "# Recall -> above 60%\n",
    "# F1-score -> above 55%\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "param_grid_rfc = {\n",
    "    'criterion': ['gini', 'log_loss'],\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_leaf': [1, 3],\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# rfc = RandomForestClassifier(class_weight=\"balanced\", n_estimators=500, random_state=42, min_samples_split=10)\n",
    "rfc = return_best_params(\"Grid\", RandomForestClassifier(), param_grid_rfc)\n",
    "rfc.fit(X_train_res, y_train_res)\n",
    "y_pred_rfc = rfc.predict(X_test)"
   ],
   "id": "495f1e215db96e93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6.2. Logistic Regression\n",
   "id": "5c966f7ce213511c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_grid_lrc = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['saga', 'liblinear', 'newton-cg', 'lbfgs'],\n",
    "    'max_iter': [2000, 5000, 10000],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "lrc = return_best_params(\"Grid\", LogisticRegression(), param_grid_lrc)\n",
    "lrc.fit(X_train_res, y_train_res)\n",
    "y_pred_lrc = lrc.predict(X_test)"
   ],
   "id": "48c10fef5edd80ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6.3. SVC\n",
   "id": "52b4a874c05f7c30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# param_grid_svc = {\n",
    "#     'C': [0.01, 0.1, 1, 10, 100],\n",
    "#     'gamma': ['scale', 'auto'],\n",
    "#     'kernel': ['linear', 'poly', 'rbf', 'sigmoid', ],\n",
    "#     'class_weight': [None, 'balanced'],\n",
    "#     'max_iter': [2000, 5000, 10000],\n",
    "#     'random_state': [42]\n",
    "# }\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_iter': [5000, 10000],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "svc = return_best_params(\"Grid\", SVC(), param_grid_svc)\n",
    "svc.fit(X_train_res, y_train_res)\n",
    "y_pred_svc = svc.predict(X_test)"
   ],
   "id": "ef02a6a153d868aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6.4 Stacking",
   "id": "fd6a0de806e53d78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "sclf = StackingClassifier(\n",
    "    estimators=[('rfc', rfc), ('lrc', lrc), ('svc', svc), ],\n",
    "    final_estimator=meta_model\n",
    ")\n",
    "sclf.fit(X_train_res, y_train_res)\n",
    "y_pred_sclf = sclf.predict(X_test)\n"
   ],
   "id": "ee08ee144208c884",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 6.5 Voting",
   "id": "fee46e60d760083d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vc = VotingClassifier(\n",
    "    estimators=[('rfc', rfc), ('lrc', lrc), ('svc', svc)],\n",
    "    voting='hard'\n",
    ")\n",
    "vc.fit(X_train_res, y_train_res)\n",
    "y_pred_vc = vc.predict(X_test)"
   ],
   "id": "177f2c4849e34fea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Performance Comparisons\n",
    "In this section, we compare the performance of the different models, using the accuracy, recall, precision, F1 and confusion matrix"
   ],
   "id": "55b027f996857f89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_metrics(option_for_average, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred, average=option_for_average, zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred, average=option_for_average, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=option_for_average, zero_division=0)\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "\n",
    "def print_metrics(accuracy, f1, precision, recall):\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "\n",
    "def convert_to_df(classifiers_names, y_preds, option_for_average=\"macro\", preprocessing_technique=\"Stem\"):\n",
    "    models = []\n",
    "    accuracy_list = []\n",
    "    f1_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    for classifier_name, y_pred in zip(classifiers_names, y_preds):\n",
    "        accuracy, f1, precision, recall = compute_metrics(option_for_average, y_pred)\n",
    "\n",
    "        models.append(classifier_name)\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_list.append(f1)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Classification name\": models,\n",
    "        \"Accuracy\": accuracy_list,\n",
    "        \"Precision\": precision_list,\n",
    "        \"Recall\": recall_list,\n",
    "        \"F1-score\": f1_list,\n",
    "        # \"Preprocessing technique\": preprocessing_technique\n",
    "    })\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def show_classification_report(algorithm_name, y_pred):\n",
    "    print(\"Results are for \", algorithm_name.__class__.__name__)\n",
    "    classification_report(y_test, y_pred, labels=np.arange(len(original_class_names)), zero_division=0)\n",
    "\n",
    "\n",
    "def convert_classification_report_to_dataframe(y_pred):\n",
    "    report_dict = classification_report(y_test, y_pred, labels=np.arange(len(original_class_names)),\n",
    "                                        target_names=original_class_names, zero_division=0, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df = report_df[report_df['support'] > 0]\n",
    "    report_df.index.name = 'category'\n",
    "    # print(report_df.to_string())\n",
    "    return report_df\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(alg, y_pred):\n",
    "    algorithm_name = alg.__class__.__name__\n",
    "    print(\"Results are for\", algorithm_name)\n",
    "    # TODO: Extract this into function\n",
    "    output_dir = \"../resources/confusion-matrices\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    file_path = os.path.join(output_dir, f\"confusion_matrix_{dataset_name}_{algorithm_name}.png\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=np.arange(len(original_class_names)))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=original_class_names,\n",
    "                yticklabels=original_class_names, cbar=True)\n",
    "\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.ylabel('True', fontsize=12)\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path, format='png')\n",
    "    plt.show()"
   ],
   "id": "5ccfbe305e673c11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Converter to Latex\n",
    "Here we use libraries to convert the obtained results directly to latex tables and graphs which can be pasted into the paper document.\n"
   ],
   "id": "19a506eff4edbf36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def convert_to_table(df: pd.DataFrame, is_classification_report=True):\n",
    "    if is_classification_report:\n",
    "        df['support'] = df['support'].astype(int)\n",
    "\n",
    "    latex_table_code = df.to_latex(\n",
    "        caption=f\"Results for {dataset_name}\",\n",
    "        label=\"tab:...\",\n",
    "        column_format=\"p{6cm}p{2cm}p{2cm}p{2cm}p{2cm}\",\n",
    "        index=True,\n",
    "        header=True,\n",
    "        float_format=\"{:0.3f}\".format\n",
    "    )\n",
    "    return latex_table_code\n"
   ],
   "id": "82cb9036b1af22a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Obtain results\n",
    "After configuring the generation of tables and classifying models, we simply pass the parameters to a function which generates the required results."
   ],
   "id": "5a769640ef334ff2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_preds = [y_pred_rfc, y_pred_svc, y_pred_vc, y_pred_sclf]\n",
    "classifiers = [\n",
    "    rfc.__class__.__name__,\n",
    "    svc.__class__.__name__,\n",
    "    vc.__class__.__name__,\n",
    "    sclf.__class__.__name__\n",
    "]\n",
    "models = [rfc, svc, vc, sclf]\n",
    "metrics_df = convert_to_df(classifiers_names=classifiers, y_preds=y_preds, option_for_average=\"macro\")\n",
    "\n",
    "latex_output = convert_to_table(metrics_df, False)\n",
    "path = '../resources/performance-metrics'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "file_path = os.path.join(path, f\"metrics_{dataset_name}.txt\")\n",
    "with open(file_path, mode='w', encoding='utf-8') as f:\n",
    "    f.write(latex_output)"
   ],
   "id": "61beb64e4ef64fbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for cls, pred in zip(models, y_preds):\n",
    "    report_dataframe = convert_classification_report_to_dataframe(pred)\n",
    "    latex_code = convert_to_table(report_dataframe)\n",
    "    path = '../resources/tables'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_path = os.path.join(path, f\"classification_report_{dataset_name}_{cls.__class__.__name__}.txt\")\n",
    "    with open(file_path, mode='w', encoding='utf-8') as f:\n",
    "        f.write(latex_code)\n",
    "    print(latex_code)"
   ],
   "id": "516eb5f4f4cd5d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for cls, pred in zip(models, y_preds):\n",
    "    plot_confusion_matrix(cls, pred)"
   ],
   "id": "3919f9a0073d3d5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Combine Results\n",
    "Here we combine all metric results, which we will plot via the violin plot."
   ],
   "id": "1f87a15843c14db2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "copy_metrics_df = metrics_df.copy()\n",
    "copy_metrics_df[\"Dataset\"] = dataset_name\n",
    "\n",
    "combined_result = pd.DataFrame(columns=[\"Dataset\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\n",
    "\n",
    "if not copy_metrics_df.empty:\n",
    "    combined_result = pd.concat([combined_result, copy_metrics_df], ignore_index=True)\n",
    "\n",
    "path = '../resources/combined-metric-results'\n",
    "file_path = os.path.join(path, \"combined_results.txt\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        first_line = file.readline()\n",
    "        if first_line.strip() == \"\":\n",
    "            results_df = pd.DataFrame(columns=[\"Dataset\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\n",
    "        else:\n",
    "            results_df = pd.read_csv(file_path, sep='\\t')\n",
    "else:\n",
    "    results_df = pd.DataFrame(columns=[\"Dataset\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\n",
    "\n",
    "if not copy_metrics_df.empty:\n",
    "    if dataset_name in results_df[\"Dataset\"].values:\n",
    "        results_df.loc[results_df[\"Dataset\"] == dataset_name, [\"Accuracy\", \"Precision\", \"Recall\",\n",
    "                                                               \"F1-score\"]] = copy_metrics_df.iloc[0, 1:].values\n",
    "    else:\n",
    "        results_df = pd.concat([results_df, copy_metrics_df], ignore_index=True)\n",
    "\n",
    "with open(file_path, mode='w', encoding='utf-8') as f:\n",
    "    results_df.to_csv(f, sep='\\t', index=False)\n",
    "\n",
    "print(results_df)\n"
   ],
   "id": "6df6a565cbfdc5a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7 Result Analysis\n",
    "Here we plot violin plots to illustrate the data distribution. We show per metric and dataset first and then per metric for the accumulated score"
   ],
   "id": "d33873b1f98e6955"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the violin plots per metric and model here\n",
    "# First we show per metric per dataset\n",
    "\n",
    "# with open('../resources/combined-metric-results/combined_results.txt', mode='r', encoding='utf-8') as f:\n",
    "#     violin_df = pd.read_csv(f, sep='\\t')\n",
    "#\n",
    "# print(violin_df)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.violinplot(data=violin_df, split=False, palette='Set2', legend='full')\n",
    "# plt.savefig('../resources/violin-plots/violin-plot.png', format='png')\n",
    "# plt.show()"
   ],
   "id": "c7282526ec6bb5dc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
