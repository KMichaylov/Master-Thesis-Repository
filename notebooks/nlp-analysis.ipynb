{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Case Study Tracing Evolutionary Changes in APIs - Preparation Phase",
   "id": "39e18150cfd283ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following case study aims at performing an analysis on 2 Java APIs. The first API is *JUnit4*, the dataset for which is already constructed from another study. The second API is AppCompat for which we need to manually construct the dataset and analyse the evolution.\n",
    "\n",
    "## Goal\n",
    "Our goal is to investigate the feasibility of the machine learning approach. This will be achieved via the following steps:\n",
    "- Read the Excel sheets and analyze the data\n",
    "- Extract important features such as the \"Changes\" column and then perform Natural Language Processing Techniques on them, such as tokenization\n",
    "- Train the algorithm to perform classification\n",
    "- Check the accuracy of the algorithm\n",
    "- In the final stage, be able to classify solely based on the \"Changes\" column\n",
    "First, we start with importing the necessary libraries and defining the file paths"
   ],
   "id": "c2ef68a00c9599dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Some implementation ideas/goals\n",
    " - Analyse for each category what are the most popular words, basically what infers that there is a Bug change, etc.\n",
    " - Try to predict the category of the change based on the trained data\n",
    " - At later stage, try the following: \"A really interesting question will be which changes impacting the architecture of the system are represented in the release log. This way you essentially combine the two RQs and the make even more sense.\" For example, are breaking changes represented in the release log, etc."
   ],
   "id": "9674425ccc84e094"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:10.581627Z",
     "start_time": "2024-12-13T12:55:10.577154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import SnowballStemmer, NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# TODO: See if this is useful\n",
    "import joblib"
   ],
   "id": "7a858da6431fc83",
   "outputs": [],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:10.610902Z",
     "start_time": "2024-12-13T12:55:10.607725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"../resources/JUnit/JUnit - Training.xlsx\"\n",
    "sheet_name = \"JUnit\""
   ],
   "id": "e704111592465856",
   "outputs": [],
   "execution_count": 203
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After defining our path, the first thing we do is to display part of our data, in order to check if everything is working correctly.",
   "id": "3b93b6a313812eff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:10.782958Z",
     "start_time": "2024-12-13T12:55:10.686340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "print(data.head())"
   ],
   "id": "2c3cc6db5c1921fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Year1  Year       Date        Version RELEASE  \\\n",
      "0  +15 years  2021 2021-02-13  4.13.  4.13.2   PATCH   \n",
      "1  +15 years  2021 2021-02-13  4.13.  4.13.2   PATCH   \n",
      "2  +15 years  2021 2021-02-13  4.13.  4.13.2   PATCH   \n",
      "3  +15 years  2021 2021-02-13  4.13.  4.13.2   PATCH   \n",
      "4  +14 years  2020 2020-10-11  4.13.  4.13.1   PATCH   \n",
      "\n",
      "                                             Changes  By          .1 Type  \\\n",
      "0                                                NaN NaN         NaN  NaN   \n",
      "1  Pull request #1687: Mark ThreadGroups created ... NaN       Rules  NaN   \n",
      "2  Pull request $1691: Only create ThreadGroups i... NaN       Rules  NaN   \n",
      "3  Pull request #1654: Fix for issue #1192: NotSe... NaN  Exceptions  NaN   \n",
      "4                                                NaN NaN         NaN  NaN   \n",
      "\n",
      "             General General Category  \n",
      "0                NaN              NaN  \n",
      "1     Fix regression          Bug fix  \n",
      "2     Fix regression          Bug fix  \n",
      "3  Fix serialization          Bug fix  \n",
      "4                NaN              NaN  \n"
     ]
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next parts, we will focus on building the classifier.\n",
    "### 1. Tokenize the \"Changes\" column\n",
    "First we clean the data from non-alphabetic characters and then perform the tokenization of the \"Changes\" column. After that we remove any stopwords, so that the output is cleaner and easier to analyse in later steps"
   ],
   "id": "3289874075af974b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:10.822451Z",
     "start_time": "2024-12-13T12:55:10.795436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import tokenizer model\n",
    "# Make sure to download the following\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "data[\"Changes\"] = data[\"Changes\"].fillna(\"\")\n",
    "print(data[\"Changes\"].head())\n",
    "data[\"Tokens\"] = data[\"Changes\"].apply(word_tokenize)\n",
    "\n",
    "# print(data[[\"Tokens\", \"Changes\"]].head())\n",
    "\n",
    "# Define the stopwords in a set.\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "data[\"Tokens\"] = data[\"Tokens\"].apply(lambda tokens: [word.lower() for word in tokens if re.match(r'^[a-zA-Z]+$',\n",
    "                                                                                                  word) and word.casefold() not in stop_words])\n",
    "print(\"Data without stop words and solely alphabetical tokens: \")\n",
    "print(data[\"Tokens\"].head())\n"
   ],
   "id": "6183a2610fcaca26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                     \n",
      "1    Pull request #1687: Mark ThreadGroups created ...\n",
      "2    Pull request $1691: Only create ThreadGroups i...\n",
      "3    Pull request #1654: Fix for issue #1192: NotSe...\n",
      "4                                                     \n",
      "Name: Changes, dtype: object\n",
      "Data without stop words and solely alphabetical tokens: \n",
      "0                                                   []\n",
      "1    [pull, request, mark, threadgroups, created, f...\n",
      "2          [pull, request, create, threadgroups, true]\n",
      "3    [pull, request, fix, issue, notserializableexc...\n",
      "4                                                   []\n",
      "Name: Tokens, dtype: object\n"
     ]
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next parts, we will focus on building the classifier.\n",
    "### 2. Stem the output\n",
    "The next step is to stem the output, the reason for doing this is to focus on the basic meaning of the word."
   ],
   "id": "fbdb6f062e586764"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:10.857375Z",
     "start_time": "2024-12-13T12:55:10.834505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Apply the stemmer\n",
    "data[\"Stemmed_Tokens\"] = data[\"Tokens\"].apply(\n",
    "    lambda tokens: [stemmer.stem(word) for word in tokens if isinstance(word, str)]\n",
    ")\n",
    "\n",
    "print(\"Stemmed tokens: \")\n",
    "print(data[[\"Tokens\", \"Stemmed_Tokens\"]].head())"
   ],
   "id": "a19fb723a52f8c74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed tokens: \n",
      "                                              Tokens  \\\n",
      "0                                                 []   \n",
      "1  [pull, request, mark, threadgroups, created, f...   \n",
      "2        [pull, request, create, threadgroups, true]   \n",
      "3  [pull, request, fix, issue, notserializableexc...   \n",
      "4                                                 []   \n",
      "\n",
      "                                      Stemmed_Tokens  \n",
      "0                                                 []  \n",
      "1  [pull, request, mark, threadgroup, creat, fail...  \n",
      "2          [pull, request, creat, threadgroup, true]  \n",
      "3  [pull, request, fix, issu, notserializableexce...  \n",
      "4                                                 []  \n"
     ]
    }
   ],
   "execution_count": 206
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Feature engineering\n",
    "After clearing the input, the next step is to extract features which will be suitable for the machine learning model. For this, we use TF-IDF (Term Frequency-Inverse Document Frequency).\n"
   ],
   "id": "ea09bf9e14d1b680"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:10.879470Z",
     "start_time": "2024-12-13T12:55:10.870541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "# TODO: Check if this is correct and makes sense\n",
    "print(data[\"Stemmed_Tokens\"].head())\n",
    "\n",
    "documents = data[\"Stemmed_Tokens\"].apply(lambda tokens: ' '.join(tokens) if isinstance(tokens, list) else '')\n",
    "\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(X.shape)"
   ],
   "id": "644ce44abc52786d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                   []\n",
      "1    [pull, request, mark, threadgroup, creat, fail...\n",
      "2            [pull, request, creat, threadgroup, true]\n",
      "3    [pull, request, fix, issu, notserializableexce...\n",
      "4                                                   []\n",
      "Name: Stemmed_Tokens, dtype: object\n",
      "(271, 514)\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Labels preparation\n",
    "Once the data is prepared, the next step is to define the lables which will be used for categorization."
   ],
   "id": "e591acc2c986dbf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:10.900314Z",
     "start_time": "2024-12-13T12:55:10.895126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_encoder = LabelEncoder()\n",
    "print(data[\"General Category\"].head())\n",
    "y = label_encoder.fit_transform(data[\"General Category\"])\n",
    "print(y)\n"
   ],
   "id": "31beeba77b5911b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        NaN\n",
      "1    Bug fix\n",
      "2    Bug fix\n",
      "3    Bug fix\n",
      "4        NaN\n",
      "Name: General Category, dtype: object\n",
      "[10  1  1  1 10  8  2 10  5  0  5  2  1  3  2  1  5  1  1  1  5  1  6  0\n",
      "  1  1  0  2  2  1  0  1  4  1  1  1  1  1  1  1  5  1  1  1  0  1  0  3\n",
      "  4  1  3  1  1  0  2  1  2  1  1  2  2  2  0  1  1  1 10  0  0  7  0  0\n",
      "  0  1  1  0  1  1  1  5  1  0  2  1  0  0  0  3  1  1  1  0  0  0  2  0\n",
      "  1  1  1  4  1  1  5  1  1  0  0  1  1  1  0  2  0  5  9  9  5  1  7 10\n",
      "  0  0  2  2  2  2  5  0  0  0  0  0 10  1  0  2  2  1  1  1  4  4  4  4\n",
      "  7  1  1  1  1  2  1  0 10  0  2  3  2  2  1  1  1  1  1  1  1  5  4  2\n",
      "  5  1 10  1 10  1 10  0  1 10  0  0  0  0  0  0  0  0  2  2  4  4  4  4\n",
      "  1  1  1 10  0  0  0  0  4  4  2  1  1 10  2  1  1  1  2  0  0  0  0  5\n",
      "  2  0  0  0  0  5  7  2  2  4  0  2  0  2 10  0  0  0  0  1  1  2  5  5\n",
      "  1  1 10  1  1 10  2  3  0  2  5  1  1  1  2  4 10  1  1  5  1 10  1  2\n",
      "  1  4  4  4  2 10  2]\n"
     ]
    }
   ],
   "execution_count": 208
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Split training and testing data\n",
    "The next step is to prepare the training and testing data for the machine learining model"
   ],
   "id": "a078e98172ea5daa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:10.936403Z",
     "start_time": "2024-12-13T12:55:10.930013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data shape: \", X_train.shape)\n",
    "print(\"Test data: \", X_test.shape)\n"
   ],
   "id": "355bfb912371cd14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (216, 514)\n",
      "Test data:  (55, 514)\n"
     ]
    }
   ],
   "execution_count": 209
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Train model\n",
   "id": "a8fabebed7dce91d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:11.178964Z",
     "start_time": "2024-12-13T12:55:10.946415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ],
   "id": "495f1e215db96e93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5636363636363636\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 10, does not match size of target_names, 11. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[210], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m classifier\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy_score(y_test, y_pred))\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClassification Report:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[43mclassification_report\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabel_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclasses_\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    214\u001B[0m         )\n\u001B[0;32m    215\u001B[0m     ):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    226\u001B[0m     )\n",
      "File \u001B[1;32mC:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2693\u001B[0m, in \u001B[0;36mclassification_report\u001B[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001B[0m\n\u001B[0;32m   2687\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   2688\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels size, \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m, does not match size of target_names, \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2689\u001B[0m                 \u001B[38;5;28mlen\u001B[39m(labels), \u001B[38;5;28mlen\u001B[39m(target_names)\n\u001B[0;32m   2690\u001B[0m             )\n\u001B[0;32m   2691\u001B[0m         )\n\u001B[0;32m   2692\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2693\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2694\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of classes, \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m, does not match size of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2695\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget_names, \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m. Try specifying the labels \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2696\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mlen\u001B[39m(labels), \u001B[38;5;28mlen\u001B[39m(target_names))\n\u001B[0;32m   2697\u001B[0m         )\n\u001B[0;32m   2698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m target_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2699\u001B[0m     target_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m l \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m labels]\n",
      "\u001B[1;31mValueError\u001B[0m: Number of classes, 10, does not match size of target_names, 11. Try specifying the labels parameter"
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### JUST A TEST",
   "id": "f6c442cfceb471a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T12:55:11.183978800Z",
     "start_time": "2024-12-13T12:52:06.218812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "joblib.dump(classifier, \"classifier_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "\n",
    "# Load the saved models\n",
    "loaded_classifier = joblib.load(\"classifier_model.pkl\")\n",
    "loaded_vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "\n",
    "# Predict on new data\n",
    "new_text = [\n",
    "    \"Source has been split into directories src/main/java and src/test/java, making it easier to exclude tests from builds, and making JUnit more maven-friendly\"]\n",
    "new_vector = loaded_vectorizer.transform(new_text)\n",
    "prediction = loaded_classifier.predict(new_vector)\n",
    "\n",
    "print(\"Predicted Category:\", label_encoder.inverse_transform(prediction))\n"
   ],
   "id": "419cbfbb9e368844",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: ['Code redesign']\n"
     ]
    }
   ],
   "execution_count": 156
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
